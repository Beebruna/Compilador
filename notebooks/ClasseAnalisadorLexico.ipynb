{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37eb27b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T15:09:19.539295Z",
     "start_time": "2022-12-06T15:09:19.532351Z"
    },
    "executionInfo": {
     "elapsed": 349,
     "status": "ok",
     "timestamp": 1669598993185,
     "user": {
      "displayName": "DÉBORA BRUNA RODRIGUES DE MORAIS",
      "userId": "17994960210651657911"
     },
     "user_tz": 180
    },
    "id": "37eb27b6"
   },
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eaafa45d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T15:09:19.803912Z",
     "start_time": "2022-12-06T15:09:19.790948Z"
    },
    "code_folding": [
     0
    ],
    "executionInfo": {
     "elapsed": 334,
     "status": "ok",
     "timestamp": 1669600475672,
     "user": {
      "displayName": "DÉBORA BRUNA RODRIGUES DE MORAIS",
      "userId": "17994960210651657911"
     },
     "user_tz": 180
    },
    "id": "eaafa45d"
   },
   "outputs": [],
   "source": [
    "class TOKEN:\n",
    "    def __init__(self, classe, lexema, tipo):\n",
    "        self.classe = classe\n",
    "        self.lexema = ''.join(lexema)\n",
    "        self.tipo = tipo\n",
    "    \n",
    "    \n",
    "    def __str__(self):\n",
    "        return f'Classe={self.classe}, lexema={self.lexema}, Tipo={self.tipo}'\n",
    "    \n",
    "        \n",
    "class AlphabetError(Exception):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b194108f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T15:09:20.162135Z",
     "start_time": "2022-12-06T15:09:20.141190Z"
    },
    "code_folding": [
     0,
     16,
     97
    ],
    "executionInfo": {
     "elapsed": 342,
     "status": "ok",
     "timestamp": 1669600999991,
     "user": {
      "displayName": "DÉBORA BRUNA RODRIGUES DE MORAIS",
      "userId": "17994960210651657911"
     },
     "user_tz": 180
    },
    "id": "b194108f"
   },
   "outputs": [],
   "source": [
    "TABELA_DE_SIMBOLOS = [\n",
    "    TOKEN('inicio', 'inicio', 'inicio'),\n",
    "    TOKEN('varinicio', 'varinicio', 'varinicio'),\n",
    "    TOKEN('varfim', 'varfim', 'varfim'),\n",
    "    TOKEN('escreva', 'escreva', 'escreva'),\n",
    "    TOKEN('leia', 'leia', 'leia'),\n",
    "    TOKEN('se', 'se', 'se'),\n",
    "    TOKEN('entao', 'entao', 'entao'),\n",
    "    TOKEN('fimse', 'fimse', 'fimse'),\n",
    "    TOKEN('fim', 'fim', 'fim'),\n",
    "    TOKEN('inteiro', 'inteiro', 'inteiro'),\n",
    "    TOKEN('literal', 'literal', 'literal'),\n",
    "    TOKEN('real', 'real', 'real')\n",
    "]\n",
    "\n",
    "\n",
    "tabela_de_transicao = {\n",
    "    0: {\n",
    "        '(': 1,\n",
    "        ')': 2,\n",
    "        ';': 3,\n",
    "        ',': 4,\n",
    "        'EOF': 5,\n",
    "        '+': 6,\n",
    "        '-': 6,\n",
    "        '*': 6,\n",
    "        '/': 6,\n",
    "        'letra': 7,\n",
    "        '\"': 8,\n",
    "        '{': 10,\n",
    "        '<': 12,\n",
    "        '>': 14,\n",
    "        '=': 15,\n",
    "        'digito': 16,\n",
    "        ' ': 0,\n",
    "        '\\n': 0,\n",
    "        '\\t': 0\n",
    "    },\n",
    "    1: {},\n",
    "    2: {},\n",
    "    3: {},\n",
    "    4: {},\n",
    "    5: {},\n",
    "    6: {},\n",
    "    7: {\n",
    "        'letra': 7,\n",
    "        'digito': 7,\n",
    "        '_': 7\n",
    "    },\n",
    "    8: {\n",
    "        'curinga': 8,\n",
    "        '\"': 9\n",
    "    },\n",
    "    9: {},\n",
    "    10: {\n",
    "        'curinga': 10,\n",
    "        '}': 11\n",
    "    },\n",
    "    11: {},\n",
    "    12: {\n",
    "        '=': 15,\n",
    "        '>': 15,\n",
    "        '-': 13\n",
    "    },\n",
    "    13: {},\n",
    "    14: {\n",
    "        '=': 15\n",
    "    },\n",
    "    15: {},\n",
    "    16: {\n",
    "        'digito': 16,\n",
    "        '.': 17,\n",
    "        'E': 19,\n",
    "        'e': 19\n",
    "    },\n",
    "    17: {\n",
    "        'digito': 18\n",
    "    },\n",
    "    18: {\n",
    "        'digito': 18,\n",
    "        'E': 19,\n",
    "        'e': 19\n",
    "    },\n",
    "    19: {\n",
    "        'digito': 21,\n",
    "        '+': 20,\n",
    "        '-': 20\n",
    "    },\n",
    "    20: {\n",
    "        'digito': 21\n",
    "    },\n",
    "    21: {\n",
    "        'digito': 21\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "estados_finais = {\n",
    "    0: 'estado inicial',\n",
    "    1: 'AB_P',\n",
    "    2: 'FC_P',\n",
    "    3: 'PT_V',\n",
    "    4: 'VIR',\n",
    "    5: 'EOF',\n",
    "    6: 'OPA',\n",
    "    7: 'id',\n",
    "    9: 'Lit',\n",
    "    11: 'Comentário',\n",
    "    12: 'OPR',\n",
    "    13: 'ATR',\n",
    "    14: 'OPR',\n",
    "    15: 'OPR',\n",
    "    16: 'Num',\n",
    "    18: 'Num',\n",
    "    21: 'Num'\n",
    "}\n",
    "\n",
    "\n",
    "letras = list(string.ascii_letters)\n",
    "digitos = list(string.digits)\n",
    "delimitadores = [' ', '\\n', '\\t'] #Perguntar a prof se fazem parte do alfabeto, pois não estão definidos no projeto\n",
    "alfabeto = list(letras + digitos + delimitadores + [\n",
    "    ',', ';', ':', '.', '!', '?', '\\\\', '*', '+', '-', '/', '(', ')', '{', '}',\n",
    "    '[', ']', '<', '>', '=', \"'\", '\"', '_'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5204b42a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T15:17:41.489579Z",
     "start_time": "2022-12-06T15:17:41.459658Z"
    },
    "code_folding": [
     27,
     42,
     89
    ],
    "id": "5204b42a"
   },
   "outputs": [],
   "source": [
    "class AnalisadorLexico:\n",
    "    \"\"\"\n",
    "    Classe usada para implementar o analisador léxico\n",
    "\n",
    "    ...\n",
    "\n",
    "    Atributos\n",
    "    ----------\n",
    "    fonte: str\n",
    "        string com o texto do código fonte\n",
    "    posicao: int\n",
    "        posição do cursor na string fonte\n",
    "    linha:\n",
    "        quantidade de \\n + 1\n",
    "    coluna:\n",
    "        posição do cursor que é zerada sempre que encontra \\n\n",
    "\n",
    "    Métodos\n",
    "    -------\n",
    "    classifica_token(self, estado, lexema, erro)\n",
    "        Classifica um token a partir do estado em que se encontra\n",
    "    chave(self, caractere, estado)\n",
    "        Retorna a chave correta para a combinação de caractere e estado sendo lidos\n",
    "    SCANNER(self)\n",
    "        Consome os caracteres do fonte e retorna um token equivalente\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, nome_arquivo):\n",
    "        \"\"\"\n",
    "        Parâmetros\n",
    "        ----------\n",
    "        nome_arquivo : str\n",
    "            nome do caminho incluindo o caminho\n",
    "        \"\"\"\n",
    "        \n",
    "        with open(nome_arquivo, 'r') as fp:\n",
    "            self.fonte = fp.read()     \n",
    "        \n",
    "        self.posicao = 0\n",
    "        self.linha = 1\n",
    "        self.coluna = 1\n",
    "        \n",
    "    def classifica_token(self, estado, lexema, erro):\n",
    "        \"\"\"Classifica um token a partir do estado em que se encontra\n",
    "\n",
    "        Parâmetros\n",
    "        ---------\n",
    "        estado: int\n",
    "            Número do estado atual referente ao autômato representado na tabela_de_transicao\n",
    "        lexema: string\n",
    "            Lexema do token\n",
    "        erro: boolean\n",
    "            Informa se foi encontrado algum erro\n",
    "\n",
    "        Retorno\n",
    "        ---------\n",
    "        Token da classe TOKEN\n",
    "        \"\"\"\n",
    "\n",
    "        if erro:\n",
    "            return TOKEN('ERROR', lexema, None)\n",
    "        \n",
    "        # Verificando se já se encontra na tabela de símbolos ou se ainda deve ser adicionado\n",
    "        elif estado == 7:\n",
    "            for token in TABELA_DE_SIMBOLOS:\n",
    "                if token.lexema == ''.join(lexema):\n",
    "                    return token\n",
    "\n",
    "            novo_id = TOKEN('id', lexema, None)\n",
    "            TABELA_DE_SIMBOLOS.append(novo_id)\n",
    "            \n",
    "            return novo_id\n",
    "        \n",
    "        # Classificação do tipo com base nos parâmetros e nas especificações do trabalho\n",
    "        elif estado == 16:\n",
    "            tipo = 'inteiro'\n",
    "\n",
    "        elif estado == 18 or estado == 21:\n",
    "            tipo = 'real'\n",
    "\n",
    "        elif estado == 9:\n",
    "            tipo = 'literal'\n",
    "\n",
    "        else:\n",
    "            tipo = None\n",
    "\n",
    "        return TOKEN(estados_finais[estado], lexema, tipo)\n",
    "\n",
    "\n",
    "    def chave(self, caractere, estado):\n",
    "        \"\"\"Retorna a chave correta para a combinação de caractere e estado sendo lidos\n",
    "\n",
    "        Essa função recebe o estado e o caractere que está sendo lido naquele estado para retornar a chave correta\n",
    "        de modo a evitar a criação de uma transição para cada caractere do alfabeto\n",
    "        Parâmetros\n",
    "        ---------\n",
    "        caractere: char\n",
    "            Caractere sendo lido\n",
    "        estado: int\n",
    "            Estado em que determinado caractere foi recebido\n",
    "\n",
    "        Retorno\n",
    "        ---------\n",
    "        chave: string\n",
    "            Argumento para a transição na tabela de transições\n",
    "        \"\"\"\n",
    "        \n",
    "        if (estado == 16 or estado == 18) and (caractere =='e' or caractere == 'E'):\n",
    "            chave = 'e'\n",
    "        elif (estado == 8 and caractere != '\"') or (estado == 10 and caractere != '}'):\n",
    "            chave = 'curinga'\n",
    "        elif caractere in letras:\n",
    "            chave = 'letra'\n",
    "        elif caractere in digitos:\n",
    "            chave = 'digito'\n",
    "        else:\n",
    "            chave = caractere\n",
    "\n",
    "        return chave\n",
    "\n",
    "    \n",
    "    def SCANNER(self):\n",
    "        \"\"\"Consome os caracteres do fonte e retorna um token equivalente\n",
    "        \n",
    "        Retorno\n",
    "        ---------\n",
    "        Token da classe TOKEN\n",
    "        \"\"\"\n",
    "        \n",
    "        estado = 0\n",
    "        lexema = []\n",
    "        erro = False\n",
    "\n",
    "        # Executa até consumir todos os caracters do fonte\n",
    "        # Python não lê o caractere EOF então a condição = para uma iteração extra que retorna o token EOF\n",
    "        while self.posicao <= len(self.fonte):\n",
    "            try:\n",
    "                if self.fonte[self.posicao] not in alfabeto:\n",
    "                    raise AlphabetError\n",
    "                    \n",
    "                estado = tabela_de_transicao[estado][self.chave(self.fonte[self.posicao], estado)]\n",
    "\n",
    "            # Caractere não encontrado no alfabeto\n",
    "            except AlphabetError:\n",
    "                if (len(lexema) > 0) and (estado in estados_finais):\n",
    "                    return self.classifica_token(estado, lexema, erro)\n",
    "                \n",
    "                erro = True\n",
    "                print(f'ERRO LÉXICO - Caractere inválido na linguagem: {self.fonte[self.posicao]}. Linha {self.linha}, coluna {self.coluna}')\n",
    "                lexema.append(self.fonte[self.posicao])\n",
    "\n",
    "            # Encontrou um caractere que quebrou o padrão e tentou realizar uma transição que não existe naquele estado\n",
    "            except KeyError:\n",
    "                # Quebrou o padrão pois chegou no fim do lexema atual\n",
    "                if (len(lexema)) > 0 and (estado in estados_finais.keys()):\n",
    "                    return self.classifica_token(estado, lexema, erro)\n",
    "                \n",
    "                # Por exemplo no lexema: 1e\n",
    "                elif estado == 19:\n",
    "                    erro = True\n",
    "                    print(f'ERRO LÉXICO - Exponenciação incompleta. Linha {self.linha}, coluna {self.coluna}') \n",
    "                    return self.classifica_token(estado, lexema, erro)\n",
    "                \n",
    "            # Chegou ao fim do fonte\n",
    "            except IndexError:\n",
    "                if len(lexema) > 0:\n",
    "                    return self.classifica_token(estado, lexema, erro)\n",
    "\n",
    "                else: \n",
    "                    return TOKEN('EOF', 'EOF', None)\n",
    "\n",
    "            # Transição ocorreu normalmente\n",
    "            else:\n",
    "                if self.fonte[self.posicao] == '\\n':\n",
    "                    self.linha = self.linha + 1\n",
    "                    self.coluna = 0\n",
    "                    \n",
    "                    if estado == 8:\n",
    "                        erro = True\n",
    "                        print(f'ERRO LÉXICO - Literal incompleto. Linha {self.linha}, coluna {self.coluna}')\n",
    "                        return self.classifica_token(estado, lexema, erro)\n",
    "                    elif estado == 10:\n",
    "                        erro = True\n",
    "                        print(f'ERRO LÉXICO - Comentário incompleto. Linha {self.linha}, coluna {self.coluna}')\n",
    "                        return self.classifica_token(estado, lexema, erro)\n",
    "                \n",
    "                # Adicionando ao lexema\n",
    "                if (self.fonte[self.posicao] not in delimitadores) or (estado == 8 or estado == 10):\n",
    "                    lexema.append(self.fonte[self.posicao])\n",
    "\n",
    "            finally:\n",
    "                self.posicao = self.posicao + 1\n",
    "                self.coluna = self.coluna + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7555da85",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-06T15:19:00.266376Z",
     "start_time": "2022-12-06T15:19:00.257400Z"
    },
    "id": "7555da85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERRO LÉXICO - Exponenciação incompleta. Linha 1, coluna 3\n",
      "Classe=ERROR, lexema=1e, Tipo=None\n",
      "Classe=Num, lexema=3, Tipo=inteiro\n",
      "Classe=Num, lexema=3, Tipo=inteiro\n",
      "Classe=id, lexema=este, Tipo=None\n",
      "Classe=Num, lexema=1, Tipo=inteiro\n",
      "Classe=Num, lexema=2, Tipo=inteiro\n",
      "Classe=id, lexema=dafdasdfa, Tipo=None\n",
      "Classe=id, lexema=saf, Tipo=None\n",
      "Classe=id, lexema=asdf, Tipo=None\n",
      "Classe=Num, lexema=1e3, Tipo=real\n",
      "Classe=EOF, lexema=EOF, Tipo=None\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    al = AnalisadorLexico('..\\\\Teste\\\\teste.txt')\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        token = al.SCANNER()\n",
    "        print(token)\n",
    "        \n",
    "        if token.classe == 'EOF':\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
